<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
  <name>http.agent.name</name>
  <value>Kairos</value>
  <description>
  </description>
</property>
 
<property>
  <name>http.agent.description</name>
  <value>crawls scholarly paper metadata</value>
  <description>
  </description>
</property>
 
<property>
  <name>http.agent.url</name>
  <value></value>
  <description>
  </description>
</property>
 
<property>
  <name>http.agent.email</name>
  <value></value>
  <description>
  </description>
</property>

<property>
  <name>db.ignore.external.links</name>
  <value>true</value>
  <description>If true, outlinks leading from a page to external hosts
  will be ignored. This is an effective way to limit the crawl to include
  only initially injected hosts, without creating complex URLFilters.
  </description>
</property>

<property>
  <name>http.useHttp11</name>
  <value>true</value>
  <description>NOTE: at the moment this works only for protocol-httpclient.
  If true, use HTTP 1.1, if false use HTTP 1.0 .
  </description>
</property>

<property>
  <name>http.content.limit</name>
  <value>1048576</value>
  <description>The length limit for downloaded content, in bytes.
  If this value is nonnegative (>=0), content longer than it will be truncated;
  otherwise, no truncation at all.
  </description>
</property>

<property>
  <name>file.content.limit</name>
  <value>10485760</value>
  <description>The length limit for downloaded content, in bytes.
  If this value is nonnegative (>=0), content longer than it will be truncated;
  otherwise, no truncation at all.
  </description>
</property>

<property>
  <name>plugin.includes</name>
  <value>kairos-plugin|protocol-http|parse-(html|pdf|msword|text|odt)|urlfilter-(regex|suffix)|urlnormalizer-(pass|regex|basic)</value>
  <description>
  </description>
</property>
</configuration>
