<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<title>Proceedings - Adversarial Information Retrieval on the Web (AIRWeb'08)</title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link rel="stylesheet" type="text/css" href="style.css" />
</head>

<body>

<div id="wrapper-menu-top">
<div id="menu-top">
	<div class="location">April 22nd, 2008 &mdash; Beijing, CHINA.</div>
        	<ul>
		<li><a href="/2008/" title="Home"><span>Home</span></a></li>
		<li><a href="proceedings.html" title="Proceedings"><span>Proceedings</span></a></li>
		<li><a href="archive.html" title="Archive"><span>Archive</span></a></li>
		<li><a href="contact.html" title="Contact"><span>Contact</span></a></li>
	</ul>

</div><!--menu-top-->
</div><!--wrapper-menu-top-->

<div id="wrapper-header">
<div id="header">
<div id="wrapper-header2">
<div id="wrapper-header3">
	<h1>AIRWeb 2008</h1>
	<h2>Fourth International Workshop on<br/><span class="authors">Adversarial Information Retrieval on the Web</h2>
</div>
</div>
</div>
</div>

<div id="wrapper-content">
<div id="wrapper-menu-page">
	<div id="menu-page">
	<h3>AIRWeb'08</h3>
        	<ul>
		<li><a href="/2008/">Home</a></li>
		<li><a href="proceedings.html">Proceedings</a></li>
		<li><a href="archive.html">Archive</a></li>
		<li><a href="contact.html">Contact</a></li>
	</ul>
	
	<h3>Past Workshops</h3>
	<ul>
		<li><a href="/2007/">AIRWeb'07<br/>Banff, Canada</a></li>
		<li><a href="/2006/">AIRWeb'06<br/>Seattle, USA</a></li>
		<li><a href="/2005/">AIRWeb'05<br/>Chiba, Japan</a></li>
	</ul>
	
	<br/>
	<p class="sponsor">Organized with:<br/><a href="http://www2008.org/"><img border="0" src="images/www2008_logo.jpg" alt="WWW2008" /></a></p>

</div><!--menu-page-->
</div>

<div id="content">

<h1>Proceedings</h1>

<p>Proceedings of the 4th international workshop on Adversarial Information Retrieval on the Web, Beijing, China, April 22nd, 2008. <a class="external" href="http://portal.acm.org/toc.cfm?id=1451983">ISBN 978-1-60558-159-0, ACM ICPS</a>.</p>

<h2>Session: Usage Analysis</h2>

<p class="paper"><a href="submissions/buehrer_2008_automated_web_search_traffic.pdf" class="pdf">A Large-scale Study of Automated Web Search Traffic</a>  &mdash; <a class="presc" href="slides/buehrer_2008_automated_web_search_traffic.pdf">slides</a><br/><span class="authors">Greg Buehrer, Jack Stokes and Kumar Chellapilla</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451985">Pages 1-8, doi.acm.org/10.1145/1451983.1451985</a> As web search providers seek to improve both relevance and response times, they are challenged by the ever-increasing tax of automated search query traffic. Third party systems interact with search engines for a variety of reasons, such as monitoring a website's rank, augmenting online games, or possibly to maliciously alter click-through rates. In this paper, we investigate automated traffic in the query stream of a large search engine provider. We define automated traffic as any search query not generated by a human in real time. We first provide examples of different categories of query logs generated by bots. We then develop many different features that distinguish between queries generated by people searching for information, and those generated by automated processes. We categorize these features into two classes, either an interpretation of the physical model of human interactions, or as behavioral patterns of automated interactions. We believe these features formulate a basis for a production-level query stream classifier.</div>

<p class="paper"><a href="submissions/liu_2008_spam_user_behavior.pdf" class="pdf">Identifying Web Spam with User Behavior Analysis</a>  &mdash; <a class="presc" href="slides/liu_2008_spam_user_behavior.pdf">slides</a><br/><span class="authors">Yiqun Liu, Rongwei Cen, Min Zhang, Liyun Ru and Shaoping Ma</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451986">Pages 9-16, doi.acm.org/10.1145/1451983.1451986</a> Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam detection techniques are usually designed for specific known types of Web spam and are incapable and inefficient for newly-appeared spam. With user behavior analyses into Web access logs, we propose a spam page detection algorithm based on Bayesian Learning. The main contributions of our work are: (1) User visiting patterns of spam pages are studied and three user behavior features are proposed to separate Web spam from ordinary ones. (2) A novel spam detection framework is proposed that can detect unknown spam types and newly-appeared spam with the help of user behavior analysis. Preliminary experiments on large scale Web access log data (containing over 2.74 billion user clicks) show the effectiveness of the proposed features and detection framework.</div>

<p class="paper"><a href="submissions/castillo_2008_query_log_detection_spam.pdf" class="pdf">Query-log mining for detecting spam</a>  &mdash; <a class="presc" href="slides/castillo_2008_query_log_detection_spam.pdf">slides</a><br/><span class="authors">Carlos Castillo, Claudio Corsi, Debora Donato, Paolo Ferragina and Aristides Gionis</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451987">Pages 17-20, doi.acm.org/10.1145/1451983.1451987</a> Every day millions of users search for information on the web via search engines, and provide implicit feedback to the results shown for their queries by clicking or not onto them. This feedback is encoded in the form of a query log that consists of a sequence of search actions, one per user query, each describing the following information: (i) terms composing a query, (ii) documents returned by the search engine, (iii) documents that have been clicked, (iv) the rank of those documents in the list of results, (v) date and time of the search action/click, (vi) an anonymous identifier for each session, and more.
<br/><br/>
In this work, we investigate the idea of characterizing the documents and the queries belonging to a given query log with the goal of improving algorithms for detecting spam, both at the document level and at the query level.</div>


<h2>Session: Text Analysis</h2>

<p class="paper"><a href="submissions/attenberg_2008_term_distance_spam.pdf" class="pdf">Cleaning Search Results using Term Distance Features</a> <a href="slides/attenberg_2008_term_distance_spam.pdf" class="presc">slides</a><br/><span class="authors">Josh Attenberg and Torsten Suel</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451989">Pages 21-24, doi.acm.org/10.1145/1451983.1451989</a> The presence of Web spam in query results is one of the critical challenges facing search engines today. While search engines try to combat the impact of spam pages on their results, the incentive for spammers to use increasingly sophisticated techniques has never been higher, since the commercial success of a Web page is strongly correlated to the number of views that page receives. This paper describes a term-based technique for spam detection based on a simple new summary data structure called Term Distance Histograms that tries to capture the topical structure of a page. We apply this technique as a post-filtering step to a major search engine. Our experiments show that we are able to detect many of the artificially generated spam pages that remained in the results of the engine. Specifically, our method is able to detect many web pages generated by utilizing techniques such as dumping, weaving, or phrase stitching [11], which are spamming techniques designed to achieve high rankings while still exhibiting many of the individual word frequency (and even bi-gram) properties of natural human text.</div>

<p class="paper"><a href="submissions/piskorski_2008_linguistic_analysis_spam.pdf" class="pdf">Exploring Linguistic Features for Web Spam Detection: A Preliminary Study</a> <a href="slides/piskorski_2008_linguistic_analysis_spam.pdf" class="presc">slides</a><br/><span class="authors">Jakub Piskorski, Marcin Sydow and Dawid Weiss</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451990">Pages 25-28, doi.acm.org/10.1145/1451983.1451990</a> We study the usability of linguistic features in the Web spam classification task. The features were computed on two Web spam corpora: Webspam-Uk2006 and Webspam-Uk2007, we make them publicly available for other researchers. Preliminary analysis seems to indicate that certain linguistic features may be useful for the spam-detection task when combined with features studied elsewhere.</div>

<p class="paper"><a href="submissions/biro_2008_latent_dirichlet_allocation_spam.pdf" class="pdf">Latent Dirichlet Allocation in Web Spam Filtering</a> <a href="slides/biro_2008_latent_dirichlet_allocation_spam.pdf" class="presc">slides</a><br/><span class="authors">Istvan Biro, Jacint Szabo and Andras Benczur</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451991">Pages 29-32, doi.acm.org/10.1145/1451983.1451991</a> Latent Dirichlet allocation (LDA) (Blei, Ng, Jordan 2003) is a fully generative statistical language model on the content and topics of a corpus of documents. In this paper we apply a modification of LDA, the novel multi-corpus LDA technique for web spam classification. We create a bag-of-words document for every Web site and run LDA both on the corpus of sites labeled as spam and as non-spam. In this way collections of spam and non-spam topics are created in the training phase. In the test phase we take the union of these collections, and an unseen site is deemed spam if its total spam topic probability is above a threshold. As far as we know, this is the first web retrieval application of LDA. We test this method on the UK2007-WEBSPAM corpus, and reach a relative improvement of 11% in F-measure by a logistic regression based combination with strong link and content baseline classifiers.</div>


<h2>Session: General</h2>

<p class="paper"><a href="submissions/sato_2008_japan_splog_characterization.pdf" class="pdf">Analysing Features of Japanese Splogs and Characteristics of Keywords</a>  &mdash; <a class="presc" href="slides/sato_2008_japan_splog_characterization.pdf">slides</a><br/><span class="authors">Yuuki Sato, Takehito Utsuro, Tomohiro Fukuhara, Yasuhide Kawada, Yoshiaki Murakami, Hiroshi Nakagawa and Noriko Kando</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451993">Pages 33-40, doi.acm.org/10.1145/1451983.1451993</a> This paper focuses on analyzing (Japanese) splogs based on various characteristics of keywords contained in them. We estimate the behavior of spammers when creating splogs from other sources by analyzing the characteristics of keywords contained in splogs. Since splogs often cause noises in word occurrence statistics in the blogosphere, we assume that we can efficiently (manually) collect splogs by sampling blog homepages containing keywords of a certain type on the date with its most frequent occurrence. We manually examine various features of collected blog homepages regarding whether their text content is excerpt from other sources or not, as well as whether they display affiliate advertisement or out-going links to affiliated sites. Among various informative results, it is important to note that more than half of the collected splogs are created by a very small number of spammers.</div>

<p class="paper"><a href="submissions/abernethy_2008_witch_content_links.pdf" class="pdf">Webspam Identification Through Content and Hyperlinks</a>  &mdash; <a class="presc" href="slides/abernethy_2008_witch_content_links.pdf">slides</a><br/><span class="authors">Jacob Abernethy, Olivier Chapelle and Carlos Castillo</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451994">Pages 41-44, doi.acm.org/10.1145/1451983.1451994</a> We present an algorithm, witch, that learns to detect spam hosts or pages on the Web. Unlike most other approaches, it simultaneously exploits the structure of the Web graph as well as page contents and features. The method is efficient, scalable, and provides state-of-the-art accuracy on a standard Web spam benchmark.</div>



<h2>Session: Social Networks</h2>

<p class="paper"><a href="submissions/benevenuto_2008_spam_video.pdf" class="pdf">Identifying Video Spammers in Online Social Networks</a> <a href="slides/benevenuto_2008_spam_video.pdf" class="presc">slides</a><br/><span class="authors">Fabricio Benevenuto, Tiago Rodrigues, Virgilio Almeida, Jussara Almeida, Chao Zhang and Keith Ross</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451996">Pages 45-52, doi.acm.org/10.1145/1451983.1451996</a> In many video social networks, including YouTube, users are permitted to post video responses to other users' videos. Such a response can be legitimate or can be a video response spam, which is a video response whose content is not related to the topic being discussed. Malicious users may post video response spam for several reasons, including increase the popularity of a video, marketing advertisements, distribute pornography, or simply pollute the system.
<br/><br/>
In this paper we consider the problem of detecting video spammers. We first construct a large test collection of YouTube users, and manually classify them as either legitimate users or spammers. We then devise a number of attributes of video users and their social behavior which could potentially be used to detect spammers. Employing these attributes, we apply machine learning to provide a heuristic for classifying an arbitrary video as either legitimate or spam. The machine learning algorithm is trained with our test collection. We then show that our approach succeeds at detecting much of the spam while only falsely classifying a small percentage of the legitimate videos as spam. Our results highlight the most important attributes for video response spam detection.</div>

<p class="paper"><a href="submissions/agichtein_2008_robust_ranking_social_media.pdf" class="pdf">A Few Bad Votes Too Many? Towards Robust Ranking in Social Media</a>  &mdash; <a class="presc" href="slides/agichtein_2008_robust_ranking_social_media.pdf">slides</a><br/><span class="authors">Eugene Agichtein, Jiang Bian, Yandong Liu, and Hongyuan Zha</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451997">Pages 53-60, doi.acm.org/10.1145/1451983.1451997</a> Online social media draws heavily on active reader participation, such as voting or rating of news stories, articles, or responses to a question. This user feedback is invaluable for ranking, filtering, and retrieving high quality content - tasks that are crucial with the explosive amount of social content on the web. Unfortunately, as social media moves into the mainstream and gains in popularity, the quality of the user feedback degrades. Some of this is due to noise, but, increasingly, a small fraction of malicious users are trying to "game the system" by selectively promoting or demoting content for profit, or fun. Hence, an effective ranking of social media content must be robust to noise in the user interactions, and in particular to vote spam. We describe a machine learning based ranking framework for social media that integrates user interactions and content relevance, and demonstrate its effec- tiveness for answer retrieval in a popular community question answering portal. We consider several vote spam attacks, and introduce a method of training our ranker to increase its robustness to some common forms of vote spam attacks. The results of our large-scale experimental evaluation show that our ranker is signifcicantly more robust to vote spam compared to a state-of-the-art baseline as well as the ranker not explicitly trained to handle malicious interactions.</div>

<p class="paper"><a href="submissions/krause_2008_anti_social_tagger.pdf" class="pdf">The Anti-Social Tagger - Detecting Spam in Social Bookmarking Systems</a> <a href="slides/krause_2008_anti_social_tagger.pdf" class="presc">slides</a><br/><span class="authors">Beate Krause, Christoph Schmitz, Andreas Hotho and Gerd Stumme</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1451998">Pages 61-68, doi.acm.org/10.1145/1451983.1451998</a> The annotation of web sites in social bookmarking systems has become a popular way to manage and find information on the web. The community structure of such systems attracts spammers: recent post pages, popular pages or specific tag pages can be manipulated easily. As a result, searching or tracking recent posts does not deliver quality results annotated in the community, but rather unsolicited, often commercial, web sites. To retain the benefits of sharing one's web content, spam-fighting mechanisms that can face the flexible strategies of spammers need to be developed.
<br/><br/>
A classical approach in machine learning is to determine relevant features that describe the system's users, train different classifiers with the selected features and choose the one with the most promising evaluation results. In this paper we will transfer this approach to a social bookmarking setting to identify spammers. We will present features considering the topological, semantic and profile-based information which people make public when using the system. The dataset used is a snapshot of the social bookmarking system BibSonomy and was built over the course of several months when cleaning the system from spam. Based on our features, we will learn a large set of different classification models and compare their performance. Our results represent the groundwork for a first application in BibSonomy and for the building of more elaborate spam detection mechanisms.</div>


<h2>Session: Link Analysis</h2>

<p class="paper"><a href="submissions/andersen_2008_robust_pagerank_local_spam.pdf" class="pdf">Robust PageRank and Locally Computable Spam Detection Features</a>  &mdash; <a class="presc" href="slides/andersen_2008_robust_pagerank_local_spam.pdf">slides</a><br/><span class="authors">Vahab Mirrokni, Reid Andersen, Christian Borgs, Jennifer Chayes, John Hopcroft, Kamal Jain and Shang-Hua Teng</span></p>
<div class="abstract"><a class="doi" href="http://doi.acm.org/10.1145/1451983.1452000">Pages 69-76, doi.acm.org/10.1145/1451983.1452000</a> Since the link structure of the web is an important element in ranking systems on search engines, web spammers widely use the link structure of the web to increase the rank of their pages. Various link-based features of web pages have been introduced and have proven effective at identifying link spam. One particularly successful family of features (as described in the SpamRank algorithm), is based on examining the sets of pages that contribute most to the PageRank of a given vertex, called supporting sets. In a recent paper, the current authors described an algorithm for efficiently computing, for a single specified vertex, an approximation of its supporting sets. In this paper, we describe several link-based spam-detection features, both supervised and unsupervised, that can be derived from these approximate supporting sets. In particular, we examine the size of a node's supporting sets and the approximate l2 norm of the PageRank contributions from other nodes. As a supervised feature, we examine the composition of a node's supporting sets. We perform experiments on two labeled real data sets to demonstrate the effectiveness of these features for spam detection, and demonstrate that these features can be computed efficiently. Furthermore, we design a variation of PageRank (called Robust PageRank) that incorporates some of these features into its ranking, argue that this variation is more robust against link spam engineering, and give an algorithm for approximating Robust PageRank.</div>


</div>
</div>


<div id="wrapper-footer">
<div id="footer">
<a href="design.html">Web Design Credits</a> | <a href="contact.html">Contact Organizers</a>
</div>
</div>

<script src="http://www.google-analytics.com/urchin.js" 
type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-721818-2";
urchinTracker();
</script>

</body>
</html>
