<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<title>ICVS 2008 Program</title>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<meta name="Author" content="Dimitris Chrysostomou">
<meta name="Abstract" content="The 6th International Conference on Computer Vision Systems (ICVS08) aims to gather researchers and developers from both academia and industry worldwide and to explore the state of the art in vision systems.">
<meta name="Description" content="The official website of the 6th International Conference in Computer Vision Systems (ICVS08)">
<meta name="Keywords" content="international, conference, computer vision systems, democritus, university, thrace, Antonis Gasteratos, Dimitris Chrysostomou, dimitris, chrysostomou, santorini, greece, 2008, gryphon lab, group robotics, cognitive systems, production management engineering, Cognitive Systems, Springer, Lecture notes, Computer Science, Robotics">
<style type="text/css">
@import"stylesheet.css";
body {
	background-image: url(images/wall.gif);
}
.style10 {
	font-size: 12px;
	color: #003366;
	font-style: italic;
}
</style>
<script src="SpryAssets/SpryMenuBar.js" type="text/javascript"></script>
<link href="SpryAssets/SpryMenuBarVertical.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="switchcontent.js" >
</script>
<style type="text/css">
/*Style sheet used for demo. Remove if desired*/
.handcursor {
	cursor:hand;
	cursor:pointer;
}
</style>
<style type="text/css">
<!--
.style11 {
	font-size: 14px;
	color: #003366;
	font-weight: bold;
}
.style13 {
	color: #003366;
	font-weight:bold;
}
.style16 {
	color: #EE0000;
	font-size: 14px;
}
.style17 {
	font-size: 14px
}
-->
</style>
</head>
<body>
<div class="clearfix mainarea">
  <div class="head"><img src="images/nlogo.jpg" alt="logo" width="700" height="100"></div>
  <div></div>
  <div class="location style10"><b>6th International Conference on Computer Vision Systems</b>, Vision for Cognitive Systems</div>
  <div class="nav">
    <ul id="MenuBar1" class="MenuBarVertical">
      <li><a href="index.htm" class="MenuBarItemSubmenu">Home</a>
        <ul>
          <li><a href="index.htm" >Home</a></li>
          <li><a href="Announcements.htm">Announcements</a></li>
          <li><a href="Venue.htm">Venue</a></li>
        </ul>
      </li>
      <li><a href="About.htm" class="MenuBarItemSubmenu">About the Conference</a>
        <ul>
          <li><a href="About.htm">About the Conference</a></li>
          <li><a href="Commitees.htm">Committees</a></li>
          <li><a href="Conference History.htm">Conference History</a></li>
          <li><a href="Contact.htm">Contact</a></li>
        </ul>
      </li>
      <li><a href="Authors Info.htm" class="MenuBarItemSubmenu">Authors Info</a>
        <ul>
          <li><a href="Authors Info.htm">Authors Info</a></li>
          <li><a href="Call for Papers.htm">Call for Papers</a></li>
          <li><a href="http://icvs2008.info/openconf/openconf.php">Paper Submission</a></li>
          <li><a href="Registration.htm">Registration</a></li>
        </ul>
      </li>
      <li><a href="Program.htm" class="MenuBarItemSubmenu">Program</a>
        <ul>
          <li><a href="Program.htm">Program</a></li>
          <li><a href="KeyNote.htm">Keynote</a></li>
          <li><a href="Workshops.htm">Workshops</a></li>
        </ul>
      </li>
      <li><a href="Touristic Info.htm" class="MenuBarItemSubmenu">Touristic Information</a>
        <ul>
          <li><a href="Touristic Info.htm">Touristic Information</a></li>
          <li><a href="Accomodation.htm">Accomodation</a></li>
          <li><a href="Transport.htm">Transport</a></li>
          <li><a href="Excursions.htm">Excursions</a></li>
          <li><a href="Useful Info.htm">Useful Information</a></li>
          <li><a href="Guide.htm">Santorini Guide</a></li>
          <li><a href="Athens Pre or Post Stays.htm">Athens Pre or Post Stays</a></li>
        </ul>
      </li>
      <li><a href="Sponsors.htm">Sponsors</a></li>
      <li><a href="PhotoGallery.htm">Photo Gallery</a></li>
    </ul>
    <div><br>
      <a href="http://www.springer.com/east/home/computer/lncs?SGWID=5-164-12-72397-0" target="_blank"><img src="images/LNCS-Logo_4c_large.gif" alt="springer" width="112" height="60" border="0"></a> </div>
    <br>
    <div><a href="http://www.eucognition.org" target="_blank"><img src="images/eucognition.gif" alt="" width="120" height="40" border="2"></a></div>
    <br>
    <div><a href="http://www.ypepth.gr/en_ec_home.htm" target="_blank">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/ypepth.jpg" alt="" width="83" height="98" border="2"></a></div>
    <br>
    <div><a href="http://poseidon.csd.auth.gr/GAIPDM/" target="_blank">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/gaipdm-logo.gif" alt="" width="60" height="60" border="0"></a></div>
    <br>
    <div><a href="http://www.duth.gr/en/" target="_blank">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/duth_head.gif" alt="" width="89" height="89" border="0"></a></div>
    <p>&nbsp;</p>
  </div>
  <div class="center">
    <div class="contenthead">
      <h1>Welcome to ICVS 2008</h1>
      <p>Vision for Cognitive Systems</p>
    </div>
    <div class="maincontent">
      <h2>Program</h2>
      <br>
      <br>
      <table width="100%" border="0" cellspacing="2" cellpadding="0">
        <tr>
          <td width="8%">&nbsp;</td>
          <td width="23%" bgcolor="#DFE9EA"><strong>12nd May <br>
            Monday</strong></td>
          <td width="23%" bgcolor="#DFE9EA"><strong>13rd May <br>
            Tuesday</strong></td>
          <td width="23%" bgcolor="#DFE9EA"><strong>14th May <br>
            Wednesday</strong></td>
          <td width="23%" bgcolor="#DFE9EA"><strong>15th May <br>
            Thursday</strong></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">8:00</div></td>
          <td rowspan="2" bgcolor="#DFE9EA">&nbsp;</td>
          <td rowspan="2" bgcolor="#DFE9EA">&nbsp;</td>
          <td rowspan="2" bgcolor="#DFE9EA">&nbsp;</td>
          <td rowspan="2" bgcolor="#DFE9EA">&nbsp;</td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">8:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">9:00</div></td>
          <td rowspan="8" valign="middle" bgcolor="#C0D8F0"><div align="center">WAPCV</div></td>
          <td rowspan="8" bgcolor="#C2DFBF"><div align="center">ICVW</div></td>
          <td rowspan="3" valign="middle" bgcolor="#FFCD9B"><div align="center"><a href="#MonitorSurveillance">Session 2<br>
              Monitor and<br>
              Surveillance</a></div></td>
          <td rowspan="4" valign="middle" bgcolor="#FFCD9B"><div align="center"><a href="#Recognition">Session 6<br>
              Object Recognition and 
              Tracking</a></div>
            <div align="center"></div>
            <div align="center"></div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">9:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">10:00</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">10:30</div></td>
          <td bgcolor="#DFE9EA">&nbsp;</td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">11:00</div></td>
          <td rowspan="4" valign="middle" bgcolor="#FFCD9B"><div align="center"><a href="#ComputerVisionArch">Session 3<br>
              Computer Vision<br>
              Architectures</a></div></td>
          <td bgcolor="#DFE9EA">&nbsp;</td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">11:30</div></td>
          <td rowspan="3" align="center" valign="middle" bgcolor="#FFCD9B"><a href="#Learning">Session 7 <br>
            Learning</a></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">12:00</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">12:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">13:00</div></td>
          <td rowspan="3" bgcolor="#DFE9EA">&nbsp;</td>
          <td rowspan="3" bgcolor="#DFE9EA">&nbsp;</td>
          <td rowspan="3" bgcolor="#DFE9EA">&nbsp;</td>
          <td rowspan="23" bgcolor="#F0F0DF">&nbsp;</td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">13:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">14:00</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">14:30</div></td>
          <td rowspan="5" bgcolor="#C0D8F0"><div align="center">WAPCV</div></td>
          <td valign="middle" bgcolor="#DFBFFF"><div align="center">Registration - Welcome</div></td>
          <td rowspan="3" valign="middle" bgcolor="#FFCD9B"><div align="center"><a href="#Calibration">Session 4<br>
              Calibration and <br>
              Registration</a></div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">15:00</div></td>
          <td rowspan="2" valign="middle" bgcolor="#FFCD9B"><div align="center"><a href="KeyNote.htm#Bulthoff" target="_self">Keynote Lecture 1</a></div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">15:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">16:00</div></td>
          <td rowspan="2" valign="middle" bgcolor="#FFCD9B"><div align="center"><a href="#Poster1">Poster Session 1</a></div></td>
          <td rowspan="2" valign="middle" bgcolor="#FFCD9B"><div align="center"><a href="#Poster2">Poster Session 2</a></div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">16:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">17:00</div></td>
          <td bgcolor="#DFE9EA">&nbsp;</td>
          <td bgcolor="#DFE9EA">&nbsp;</td>
          <td bgcolor="#DFE9EA">&nbsp;</td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">17:30</div></td>
          <td rowspan="5" bgcolor="#C2DFBF"><div align="center">ICVW</div></td>
          <td rowspan="5" valign="middle" bgcolor="#FFCD9B"><div align="center"><a href="#CognitiveVision">Session 1<br>
              Cognitive Vision</a></div></td>
          <td rowspan="2" align="center" valign="middle" bgcolor="#FFCD9B"><div align="center"><a href="KeyNote.htm#Hogg">Keynote Lecture 2</a></div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">18:00</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">18:30</div></td>
          <td rowspan="3" align="center" valign="middle" bgcolor="#FFCD9B"><a href="#CrossModal">Session 5<br>
            Cross Modal Systems</a></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">19:00</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">19:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">20:00</div></td>
          <td rowspan="9" bgcolor="#DFE9EA">&nbsp;</td>
          <td rowspan="2" bgcolor="#DFBFFF"><div align="center">Welcome Event</div></td>
          <td rowspan="2" bgcolor="#DFE9EA">&nbsp;</td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">20:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">21:00</div></td>
          <td rowspan="7" bgcolor="#DFE9EA">&nbsp;</td>
          <td rowspan="7" bgcolor="#DFBFFF"><div align="center">Banquet</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">21:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">22:00</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">22:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">23:00</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">23:30</div></td>
        </tr>
        <tr>
          <td bgcolor="#DFE9EA"><div align="center">0:00</div></td>
        </tr>
      </table>
      <p>&nbsp;</p>
      <p><span class="style16">(NEW!)</span> <a href="downloads/ICVS08_Programme.pdf" class="style17">Download the ICVS08 final program in pdf format.</a><img src="images/ico_pdf.gif" alt="flyer" width="16" height="16"></p>
      <p><span class="style11">Sessions</span> </p>
      <p><span class="style13">ICVS Poster Sessions</span></p>
      <ul class="bullet">
        <li>
          <h3 id="joecontent25-title" class="handcursor"><a name="Poster1"></a>Poster Session 1 / Chair: Rita Cucchiara</h3>
          <br>
          <p id="joecontent25" class="switchgroup2"> <strong>A Segmentation Approach In Novel Real Time 3D Plant Recognition System</strong><br>
            Dejan Seatovic<br>
            <strong>A Tale of Two Object Recognition Methods for Mobile Robots</strong><br>
            Arnau Ramisa,Shrihari Vasudevan,Ramon L&oacute;pez de M&aacute;ntaras,Roland Siegwart<br>
            <strong>Automatic Object Detection On Aerial Images Using Local Descriptors and Image Synthesis</strong><br>
            Xavier Perrotton, Marc Sturzel, Michel Roux<br>
            <strong>Bottom-Up and Top-Down Object Matching Using Asynchronous Agents and a contrario Principles</strong><br>
            Nicolas Burrus, Thierry Bernard, Jean-Michel Jolion<br>
            <strong>CEDD: Color and Edge Directivity Descriptor. A Compact Descriptor for Image Indexing and Retrieval</strong><br>
            Savvas Chatzichristofis, Yiannis Boutalis<br>
            <strong>Face Recognition Using a Color PCA Framework</strong><br>
            Mani Thomas, Senthil Kumar, Chandra Kambhamettu<br>
            <strong>Multiscale Laplacian Operators for Feature Extraction on Irregularly Distributed 3-D Range Data</strong><br>
            Shanmugalingam Suganthan, Sonya Coleman, Bryan Scotney<br>
            <strong>Online Learning for Bootstrapping of Object Recognition and Localization in a Biologically Motivated Architecture</strong><br>
            Heiko Wersing, Stephan Kirstein, Bernd Schneiders, Ute Bauer-Wersing, Edgar Koerner<br>
            <strong>Ranking Corner Points by the Angular Difference between Dominant Edges</strong><br>
            Rafael Lemuz, Miguel Arias<br>
            <strong>Scene Classification Based on Multi-resolution Orientation Histogram of Gabor Features</strong><br>
            Kazuhiro Hotta<br>
            <strong>Skeletonization Based on Metrical Neighborhood Sequences</strong><br>
            Attila Fazekas, K&aacute;lm&aacute;n Pal&aacute;gyi, G&aacute;bor N&eacute;meth, Gy&ouml;rgy Kov&aacute;cs<br>
            <strong>Vein Segmentation in Infrared Images Using Compound Enhancing and Crisp Clustering</strong><br>
            Marios Vlachos, Evangelos Dermatas<br>
            <strong>Weighted Dissociated Dipoles: An Extended Visual Feature Set</strong><br>
            Xavier Bar&oacute;, Jordi Vitri&agrave;<br>
            <strong>Rk-means: A k-means Based Clustering Algorithm</strong><br>
            Domenico Daniele Bloisi, Luca Iocchi<br>
            <strong>Smoke Detection in Video Surveillance: a MoG Model in the Wavelet Domain</strong><br>
            Simone Calderara, Paolo Piccinini, Rita Cucchiara</p>
        </li>
        <li>
          <h3 id="joecontent26-title" class="handcursor"><a name="Poster2"></a>Poster Session 2 / Chair: Petia Radeva</h3>
          <br>
          <p class="switchgroup2" id="joecontent26"> <strong>Covert Attention with a Spiking Neural Network</strong><br>
            Sylvain Chevallier, Philippe Tarroux<br>
            <strong>Enhancing Robustness of a Saliency-based Attention System for Driver Assistance</strong><br>
            Thomas Michalke, Jannik Fritsch, Christian Goerick<br>
            <strong>Salient Region Detection and Segmentation</strong><br>
            Radhakrishna Achanta, Francisco Estrada, Patricia Wils, Sabine S&uuml;sstrunk<br>
            <strong>A Novel Feature Selection based Semi-Supervised method for Image Classification<br>
            </strong>Muhammad  Tahir, Edward Smith, Praminda Caleb-Solly<br>
            <strong>Increasing Classification Robustness with Adaptive Features</strong><br>
            Christian Eitzinger, Manfred Gmainer, Wolfgang Heidl, Edwin Lughofer <br>
            <strong>Learning Contextual Variations for Video Segmentation<br>
            </strong>Vincent Martin, Monique Thonnat<br>
            <strong>Learning to Detect Aircrafts at Low Resolutions<br>
            </strong>Stavros Petridis, Christopher Geyer, Sanjiv Singh<br>
            <strong>Learning Visual Quality Inspection from Multiple Humans using Ensembles of Classifiers<br>
            </strong>Davy Sannen, Hendrik Van Brussel, Marnix Nuttin<br>
            <strong>Sub-class Error-Correcting Output Codes</strong><br>
            Sergio Escalera, Oriol Pujol, Petia Radeva <br>
            <strong>Automatic Initialization for Facial Analysis in Interactive Robotics</strong><br>
            Ahmad Rabie, Christian Lang, Marc Hahnheide, Modesto Castrillon Santana, Gerhard Sagerer <br>
            <strong>Face Recognition across Pose Using View Based Active Appearance Models (VBAAMs) on CMU Multi-PIE Dataset</strong><br>
            Jingu Heo, Marios Savvides <br>
            <strong>Spatio-temporal 3D Pose Estimation of Objects in Stereo Images</strong><br>
            Bjoern Barrois, Christian Woehler<br>
            <strong>An On-Line Interactive Self-Adaptive Image Classification Framework</strong><br>
            Davy Sannen, Marnix Nuttin, Praminda Caleb-Solly, Jim Smith, Muhammad Tahir<br>
            <strong>Communication-Aware Face Detection Using Noc Architecture</strong><br>
            Hung-Chih Lai, Marios Savvides, Tsuhan Chen<br>
            <strong>Open Source Multiview Reconstruction and Evaluation</strong><br>
            Keir Mierle, James MacLean </p>
        </li>
      </ul>
      <p><span class="style13"><a name="CognitiveVision"></a>Session 1 - Cognitive Vision / Chair: B&auml;rbel Mertsching</span><br>
        Tuesday, 13th May , 17:30-19:10</p>
      <ul class="bullet">
        <li>
          <h3 id="joecontent1-title" class="handcursor">Visual search in static and dynamic scenes using fine-grain top-down visual attention</h3>
          <p id="joecontent1" class="switchgroup2">Artificial visual attention is one of the key methodologies inspired from nature that can lead to robust and efficient visual search by machine vision systems. A novel approach is proposed for modeling of top-down visual attention in which separate saliency maps for the two attention pathways are suggested. The maps for the bottom-up pathway are built using unbiased rarity criteria while the top-down maps are created using fine-grain feature similarity with the search target as suggested by the literature on natural vision. The model has shown robustness and efficiency during experiments on visual search using natural and artificial visual input under static as well as dynamic scenarios. </p>
          Muhammad Zaheer Aziz, B&auml;rbel Mertsching </li>
        <li>
          <h3 id="joecontent2-title" class="handcursor">Integration of Visual and Shape Attributes for Object Action Complexes</h3>
          <p id="joecontent2" class="switchgroup2"> Our work is oriented towards the idea of developing cognitive
            capabilities in artificial systems through Object Action Complexes (OACs) [10].
            The theory comes up with the claim that objects and actions are inseparably
            intertwined. Categories of objects are not built by visual appearance only, as
            very common in computer vision, but by the actions an agent can perform and
            by attributes perceivable. The core of the OAC concept is constituting objects
            from a set of attributes, which can be manifold in type (e.g. color, shape, mass,
            material), to actions. This twofold of attributes and actions provides the base
            for categories. The work presented here is embedded in the development of an
            extensible system for providing and evolving attributes, beginning with attributes
            extractable from visual data.</p>
          Kai Huebner, M&aring;rten Bj&ouml;rkman, Babak Rasolzadeh, Martina Schmidt, Danica Kragic </li>
        <li>
          <h3 id="joecontent3-title" class="handcursor">3D Action Recognition and Long-term Prediction of Human Motion</h3>
          <p id="joecontent3" class="switchgroup2"> In this contribution we introduce a novel method for 3D trajectory based recognition of working actions and long-term motion prediction. The 3D pose of the human hand-forearm limb is tracked over time with a multi-hypothesis Kalman Filter framework using the Multiocular Contracting Curve Density algorithm (MOCCD) as a 3D pose estimation method. A novel trajectory classification approach is introduced which relies on the Levenshtein Distance on Trajectories (LDT) as a measure for the similarity between trajectories. Experimental investigations are performed on 10 real-world test sequences acquired from different viewpoints in a working environment. The system performs the simultaneous recognition of a working action and a cognitive long-term motion prediction. Trajectory recognition rates around 90% are achieved, requiring only a small number of training sequences. The proposed prediction approach yields significantly more reliable results than a Kalman Filter based reference approach.</p>
          Markus Hahn, Christian Woehler, Lars Krueger, </li>
        <li>
          <h3 id="joecontent4-title" class="handcursor"> Tracking of Human Hands and Faces through Probabilistic Fusion of Multiple Visual Cues</h3>
          <p id="joecontent4" class="switchgroup2">This paper describes a novel approach for real time segmentation and tracking of human hands and faces on image sequences. The proposed method builds on our previous research on color-based, skin-color tracking, eliminating inherent limitations of the existing approach such as its inability to distinguish between human hands, faces and other skin-colored regions in the background. To overcome these limitations, the proposed approach allows the utilization of additional information cues including motion information given by means of a background substraction algorithm, and top-down information regarding the formed image segments such as their spatial location, velocity and shape. All information cues are combined under a probabilistic framework which furnishes the proposed approach with the ability to cope with uncertainty due to noise. The proposed approach runs in real time on a standard, personal computer. Experimental results presented in this paper, confirm the effectiveness of the proposed methodology and its advantages over previous approaches.</p>
          Haris Baltzakis, Antonis Argyros, Manolis Lourakis, Panos Trahanias<br>
          <br>
        </li>
      </ul>
      <p><span class="style13"><a name="MonitorSurveillance"></a>Session 2 - Monitor and Surveilance / Chair: James Ferryman </span><br>
        Wednesday, 14th May , 09:00-10:15</p>
      <ul class="bullet">
        <li>
          <h3 id="joecontent5-title" class="handcursor">The SAFEE On-Board Threat Detection System</h3>
          <p id="joecontent5" class="switchgroup2">Under the framework of the European Union Funded SAFEE project, this paper gives an overview of a novel monitoring and scene analysis system developed for use onboard aircraft in spatially constrained environments. The techniques discussed herein aim to warn on-board crew about pre-determined indicators of threat intent (such as running or shouting in the cabin), as elicited from industry and security experts. The subject matter experts belive that activities such as these are strong indicators of the beginnings of undesirable chains of events, which should not be allowed to develop aboard aircraft. These events may lead to situations involving unruly passengers or be indicative of the precursors to terrorist threats. With a state of the art tracking system using homography intersections of motion images, and probability based Petri nets for scene understanding, the SAFEE behavioural analysis system automatically assesses the output from multiple intelligent sensors, and creates recommendations that are presented to the crew using an integrated airborn user interface. Evaluation of the system is conducted within a full size aircraft mockup, and experimental results are presented, showing that the SAFEE system is well suited to monitoring people in confined environments, and that meaningful and instructive output regarding human intentions can be derived from the sensor network within the cabin.</p>
          Nicholas Carter, James Ferryman </li>
        <li>
          <h3 id="joecontent6-title" class="handcursor">Region of Interest Generation in Dynamic Environments Using Local Entropy Fields</h3>
          <p id="joecontent6" class="switchgroup2">This paper presents a novel technique to generate regions of interest in image sequences containing independent motions. The technique uses a novel motion segmentation method to detect independent relative motions using a local entropies field. Local entropy values are computed for each vector of the optical flow with respect to their neighborhood. These values are used as input vectors for a two state Markov Random Field that is used to discriminate the boundaries of the clusters. The idea is to exploit the local entropy values as highly informative cues about the amount of information contained in the vector's neighborhood. High values represent significative motion differences, low values express uniform movements. After a graph cutting labeling, the cluster motion information is used to create a multiple hypothesis prediction for the following frame based on nearest neighbor greedy data association technique. The algorithm can be used as an hypothesis generator on area of interest because it already gives results within two frames. In order to show the validity of the proposed algorithm experiments have been performed in standard datasets and real-world outdoor environment showing promising results.</p>
          Luciano Spinello, Roland Siegwart </li>
        <li>
          <h3 id="joecontent7-title" class="handcursor">Real-time Face Tracking for Attention Aware Adaptive Games</h3>
          <p id="joecontent7" class="switchgroup2">This paper presents a real time face tracking and head pose estimation system which is included in an attention aware game framework. This fast tracking system enables the detection of the player&rsquo;s attentional state using a simple attention model. This state is then used to adapt the game unfolding in order to enhance user&rsquo;s experience (in the case of adventure game) and improve the game attentional attractiveness (in the case of pedagogical game).</p>
          Matthieu Perreira Da Silva, Vincent Courboulay, Armelle Prigent, Pascal Estraillier<br>
          <br>
        </li>
      </ul>
      <p><span class="style13"><a name="ComputerVisionArch"></a>Session 3 - Computer Vision Architectures / Chairs: Marios Savvides, Markus Vincze</span><br>
        Wednesday, 14th May , 11:00-12:40</p>
      <ul class="bullet">
        <li>
          <h3 id="joecontent9-title" class="handcursor">Feature Extraction and Classification by Genetic Programming</h3>
          <p id="joecontent9" class="switchgroup2"> This paper explores the use of genetic programming for constructing vision systems. A two-stage approach is used, with separate evolution of the feature extraction and classification stages. The strategy taken for the classifier is to evolve a set of partial solutions, each of which works for a single class. It is found that this approach is significantly faster than conventional genetic programming, and frequently results in a better classifier. The effectiveness of the approach is explored on three image classification problems.</p>
          Olly Oechsle, Adrian Clark </li>
        <li>
          <h3 id="joecontent10-title" class="handcursor">GPU-based Multigrid: Real-Time Performance in High Resolution Nonlinear Image Processing</h3>
          <p id="joecontent10" class="switchgroup2">Multigrid methods provide fast solvers for a wide variety of problems encountered in computer vision. Recent graphics hardware is ideally suited for the implementation of such methods, but this potential has not yet been fully realized. Typically, work in that area focuses on linear systems only, or on implementation of numerical solvers that are not as efficient as multigrid methods. We demonstrate that nonlinear multigrid methods can be used to great effect on modern graphics hardware. Specifically, we implement two applications: a nonlinear denoising filter and a solver for variational optical flow. We show that performing these computations on graphics hardware is between one and two orders of magnitude faster than comparable CPU-based implementations.</p>
          Harald Grossauer, Peter Thoman </li>
        <li>
          <h3 id="joecontent11-title" class="handcursor">Attention Modulation using Short- and Long-term Knowledge</h3>
          <p id="joecontent11" class="switchgroup2">A fast and reliable visual search is crucial for representing visual scenes. Here the modulation of bottom-up attention plays an important role. Often the knowledge about target features is used to bias the bottom-up pathway. In this paper we propose a system which does not only make use of knowledge about the target features, but also uses already acquired knowledge about objects in the current scene to speed up the visual search. Main ingredients are a relational short term memory in connection with a semantic relational long term memory and an adjustable bottom-up saliency. The focus of this work is to investigate mechanisms to use the memory of the system efficiently. We show a proof-of-concept implementation which is working in a real-world environment and performs visual search tasks. It becomes clear that using the relational semantic memory in combination with spatial and feature modulation of the bottom-up path is beneficial for speeding up such search tasks.</p>
          Sven Rebhan, Florian Roehrbein, Julian Eggert, Edgar Koerner </li>
        <li>
          <h3 id="joecontent12-title" class="handcursor">PCA Based 3D shape Reconstruction of Human Foot Using Multiple Viewpoint Cameras</h3>
          <p id="joecontent12" class="switchgroup2">This article describes a multiple camera based method to reconstruct a 3D shape of a human foot. From a feet database, an initial 3D model of the foot represented by a cloud of points is built. In addition, some shape parameters, which characterize any foot at more than 92%, are defined by using Principal Component Analysis. Then, the 3D model is adapted to the foot of interest captured in multiple images based on &quot;active shape models&quot; methods by applying some constraints (edge points' distance, color variance for example). We insist here on the experiment part where we demonstrate the efficiency of the proposed method on a plastic foot model, and on real human feet with various shapes. We compare different ways to texture the foot, and conclude that using projectors can improve drastically the reconstruction's accuracy. Based on experimental results, we finally propose some improvements regarding to the system integration.</p>
          Edmee Amsutz, Tomoaki Teshima, Makoto Kimura, Masaaki Mochimaru, Hideo Saito <br>
          <br>
        </li>
      </ul>
      <p><span class="style13"><a name="Calibration"></a>Session 4 - Calibration and Registration / Chair: Fiora Pirri </span><br>
        Wednesday, 14th May , 14:30-15:45</p>
      <ul class="bullet">
        <li>
          <h3 id="joecontent13-title" class="handcursor">A System for Geometrically Constrained Single View Reconstruction</h3>
          <p id="joecontent13" class="switchgroup2">This paper presents an overview of a system for recovering 3D models corresponding to scenes for which only a single perspective image is available. The system encompasses a versatile set of semi-automatic single view reconstruction techniques and couples them with limited interactive user input in order to reconstruct textured 3D graphical models corresponding to the imaged input scenes. Such 3D models can serve as the digital content for supporting interactive multimedia and virtual reality applications. Furthermore, they can support novel applications in areas such as video games, 3D photography, visual metrology, computer-assisted study of art and forensics, etc. </p>
          Manolis Lourakis </li>
        <li>
          <h3 id="joecontent14-title" class="handcursor">Monocular Omnidirectional Visual Odometry for Outdoor Ground Vehicles</h3>
          <p id="joecontent14" class="switchgroup2">In this paper, we describe a real-time algorithm for computing the ego-motion of a vehicle relative to the road. The algorithm uses as only input images provided by a single omnidirectional camera mounted on the roof of the vehicle. The front ends of the system are two different trackers. The first one is a homography-based tracker that detects and matches robust scale invariant features that most likely belong to the ground plane. The second one uses an appearance based approach and gives high resolution estimates of the rotation of the vehicle. This 2D pose estimation method has been successfully applied to videos from an automotive platform. We give an example of camera trajectory estimated purely from omnidirectional images over a distance of 450 meters. For performance evaluation, the estimated path is superimposed onto a Google Earth image of the same test environment. In the end, we use image mosaicing to obtain a textured 2D reconstruction of the estimated path.</p>
          Davide Scaramuzza, Roland Siegwart </li>
        <li>
          <h3 id="joecontent15-title" class="handcursor">Eyes and Cameras Calibration for 3d World Gaze Detection</h3>
          <p id="joecontent15" class="switchgroup2">Gaze tracking is a promising research area with application that goes from advanced human machine interaction systems, to human attention processes studying, modeling and use in cognitive vision fields. In this paper we propose a novel approach for the calibration and use of a head mounted dual eye gaze tracker. Key aspects are a robust pupil tracking algorithm based on prediction from infrared LED purkinje image position, and a new gaze localization method based on 3D lines of sight computation, and trifocal geometry considerations.</p>
          Stefano Marra, Fiora Pirri<br>
          <br>
        </li>
      </ul>
      <p><span class="style13"><a name="CrossModal"></a>Session 5 - Cross Modal Systems / Chair: Monique Thonnat</span><br>
        Wednesday, 14th May , 18:30-19:45</p>
      <ul class="bullet">
        <li>
          <h3 id="joecontent23-title" class="handcursor">Object Category Detection using Audio-visual Cues</h3>
          <p id="joecontent23" class="switchgroup2">Categorization is one of the fundamental building blocks of cognitive systems. Object categorization has traditionally been addressed in the vision domain, even though cognitive agents are intrinsically multimodal. Indeed, biological systems combine several modalities in order to achieve robust categorization. In this paper we propose a multimodal approach to object category detection, using audio and visual information. The auditory channel is modeled on biologically motivated spectral features via a discriminative classifier. The visual channel is modeled by a state of the art part based model. Multimodality is achieved using two fusion schemes, one high level and the other low level. Experiments on six different object categories, under increasingly difficult conditions, show strengths and weaknesses of the two approaches, and clearly underline the open challenges for multimodal category detection. </p>
          Jie Luo, Barbara Caputo, Alon Zweig, Joerg-Hendrik Bach, Joern Anemueller </li>
        <li>
          <h3 id="joecontent24-title" class="handcursor">Multimodal Interaction Abilities for a Robot Companion</h3>
          <p id="joecontent24" class="switchgroup2">Among the cognitive abilities a robot companion must be endowed with, human perception and speech understanding are both fundamental in the context of multimodal human-robot interaction. The two components we have developed and integrated on a mobile robot are presented in this paper. First, we detail an interactively distributed multiple object tracker dedicated to two-handed gestures and head location in 3D. The on-board speech understanding system is then depicted. For both components, associated in- and off-line evaluations from data acquired by the robot highlight their relevance. Implementation and preliminary experiments on a household robot companion are then demonstrated. The latter illustrate how vision can assist speech by specifying location references, object/person IDs in verbal statements in order to interpret natural deictic commands given by human beings. Extensions of our work are finally discussed.</p>
          Brice Burger, Isabelle Feranne, Frederic Lerasle </li>
      </ul>
      <br>
      <br>
      <p><span class="style13"><a name="Recognition"></a>Session 6 - Object Recognition and Tracking / Chair: Bernd Neumann</span><br>
        Thursday, 15th May , 09:00-10:40</p>
      <ul class="bullet">
        <li>
          <h3 id="joecontent16-title" class="handcursor">Diagnostic System for Intestinal Motility Disfunctions Using Video Capsule Endoscopy</h3>
          <p id="joecontent16" class="switchgroup2">Wireless Video Capsule Endoscopy is a clinical technique consisting of the analysis of images from the intestine which are provided by an ingestible device with a camera attached to it. In this paper we propose an automatic system to diagnose severe intestinal motility disfunctions using the video endoscopy data. The system is based on the application of computer vision techniques within a machine learning framework in order to obtain the characterization of diverse motility events from video sequences. We present experimental results that demonstrate the effectiveness of the proposed system and compare them with the ground-truth provided by the gastroenterologists.</p>
          Santi Segu&iacute;, Laura Igual, Fernando Vilari&ntilde;o, Petia Radeva, Carolina Malagelada </li>
        <h3 id="joecontent17-title" class="handcursor">Detecting and Recognizing Abandoned Objects in Crowded Environments</h3>
        <p id="joecontent17" class="switchgroup2">In this paper we present a framework for detecting and recognizing abandoned objects in crowded environments. The two main components of the framework include background change detection and object recognition. Moving blocks are detected using dynamic thresholding of spatiotemporal texture changes. The background change detection is based on analyzing wavelet transform coefficients of non-overlapping and non-moving 3D texture blocks. Detected changed background becomes the region of interest which is scanned to recognize various objects under surveillance such as abandoned luggage. The object recognition is based on model histogram ratios of image gradient magnitude patches. Supervised learning of the objects is performed by support vector machine. Experimental results are demonstrated using various benchmark video sequences (PETS, CAVIAR, i-Lids) and an object category dataset (CalTech256).</p>
        Roland Miezianko, Dragoljub Pokrajac
        </li>
        <li>
        <li>
          <h3 id="joecontent18-title" class="handcursor">An Approach for Tracking the 3D Object Pose Using Two Object Points</h3>
          <p id="joecontent18" class="switchgroup2">In this paper, a novel and simple approach for tracking the object pose, position and orientation, using two object points when the object is rotated about one of the axes of the reference coordinate system is presented. The object rotation angle can be tracked up to a range of 180&deg; for object rotations around each axis of the reference coordinate system from an initial object situation. The considered two object points are arbitrary points of the object which can be uniquely identified in stereo images. Since the approach requires only two object points, it is advantageous for the robotic applications where very few feature points can be obtained because of lack pattern information on the objects. The paper also presents the results for the pose estimation of a meal tray in a rehabilitation robotics environment</p>
          Sai Krishna Vuppala, Axel Gr&auml;ser </li>
        <li>
          <h3 id="joecontent19-title" class="handcursor">Adaptive Motion-based Gesture Recognition Interface for Mobile Phones</h3>
          <p id="joecontent19" class="switchgroup2">In this paper, we introduce a new vision based interaction technique for mobile phones. The user operates the interface by simply moving a finger in front of a camera. During these movements the finger is tracked using a method that embeds the Kalman filter and Expectation Maximization (EM) algorithms. Finger movements are interpreted as gestures using Hidden Markov Models (HMMs). This involves first creating a generic model of the gesture and then utilizing unsupervised Maximum a Posteriori (MAP) adaptation to improve the recognition rate for a specific user. Experiments conducted on a recognition task involving simple control commands clearly demonstrate the performance of our approach.</p>
          Jari Hannuksela, Mark Barnard, Pekka Sangi, Janne Heikkil&auml;<br>
          <br>
        </li>
      </ul>
      <p><span class="style13"><a name="Learning"></a>Session 7 - Learning / Chair: Antonios Argyros </span><br>
        Thursday, 15th May , 11:20-12:35</p>
      <ul class="bullet">
        <li>
          <h3 id="joecontent20-title" class="handcursor">A System that Learns to Tag Videos by Watching Youtube</h3>
          <p id="joecontent20" class="switchgroup2">We present a system that automatically tags videos, i.e. detects high-level semantic concepts like objects or actions in them. To do so, our system does not &ndash; in contrast to previous work &ndash; rely on datasets manually annotated for research purposes. Instead, we propose to use videos from online portals like youtube.com as a novel source of training data, where tags provided by users during upload serve as ground truth annotations. This allows our system to learn autonomously by automatically downloading its training set. The key contribution of this work is a number of large-scale quantitative experiments on real-world online videos, in which we investigate the influence of the individual system components, and how well our tagger generalizes to novel content. Our key results are: (1) Fair tagging results can be obtained by a late fusion of several kinds of visual features. (2) Using more than one keyframe per shot is helpful. (3) To generalize to different video content (e.g., another video portal), the system can be adapted by expanding its training set.</p>
          Adrian Ulges, Christian Schulze, Daniel Keysers, Thomas M. Breuel </li>
        <li>
          <h3 id="joecontent21-title" class="handcursor">Geo-located Image Grouping Using Latent Descriptions</h3>
          <p id="joecontent21" class="switchgroup2">Image categorization is undoubtedly one of the most challenging problems faced in Computer Vision. The scientific literature is plenty of methods dedicated to specific classes of images; further, commercial systems are also going to be advertised in the market. Nowadays, additional data can also be associated to the images, enriching its semantic interpretation beyond the pure appearance. This is the case of geo-location data, that contain information about the geographical place where an image has been captured. This data allow, if not require, a different management of the images, for instance, to the purpose of easy retrieval and visualization from a geo-referenced image repository. This paper constitutes a first step in this sense, presenting a method for geo-referenced image categorization. The solution presented here places in the wide literature on the statistical latent descriptions, where the probabilistic Latent Semantic Analysis (pLSA) is one of the most known representative. In particular, we extend the pLSA paradigm, introducing a latent variable modelling the geographical area in which an image has been captured. In this way, we are able to describe the entire image dataset grouping effectively proximal images with similar appearance. Experiments on categorization have been carried out, employing a wellknown geographical image repository: results are actually very promising, opening new interesting challenges and applications in this research field.</p>
          Marco Cristani, Vittorio Murino </li>
        <li>
          <h3 id="joecontent22-title" class="handcursor">Functional Object Class Detection Based on Learned Affordance Cues</h3>
          <p id="joecontent22" class="switchgroup2">Current approaches to visual object class detection mainly focus on the recognition of abstract object categories, such as cars, motorbikes, mugs and bottles. Although these approaches have demonstrated impressive performance in terms of recognition, their restriction to abstract categories seems artificial and inadequate in the context of embodied, cognitive agents. Here, distinguishing objects according to functional aspects based on object affordances is vital for a meaningful human-machine interaction. In this paper, we propose a complete system for the detection of functional object classes, based on a representation of visually distinct hints on object affordances (affordance cues). It spans the complete cycle from tutor-driven acquisition of affordance cues, one-shot learning of corresponding object models, and detecting novel instances of functional object classes in real images.</p>
          Michael Stark, Philipp Lies, Michael Zillich, Bernt Schiele <br>
          <br>
        </li>
      </ul>
      <script type="text/javascript">

var joeexample=new switchcontent("switchgroup2", "p") 
joeexample.setStatus('[-] ', '[+] ')
joeexample.setColor('black', '#003366')
joeexample.collapsePrevious(false)
joeexample.setPersist(false)
joeexample.init()
</script>
      <p>&nbsp;</p>
    </div>
  </div>
</div>
<div class="footer">
  <div align="center">This site is maintained by the <a href="http://robotics.pme.duth.gr">Group of Robotics and Cognitive Systems</a><br>
  </div>
</div>
<script type="text/javascript">
<!--
var MenuBar1 = new Spry.Widget.MenuBar("MenuBar1", {imgRight:"SpryAssets/SpryMenuBarRightHover.gif"});
//-->
</script>
</a>
</body>
</html>
